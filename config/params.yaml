project: "rnaseq-mini"
engine: "snakemake"        # snakemake or nextflow
se: false                   # single-end reads if true
threads: 4
memory_gb: 32
organism: "yeast"
reference:
  transcripts_fa: "references/yeast/transcripts.fa.gz"
  annotation_gtf: "references/yeast/annotation.gtf.gz"
  decoy_fasta: "references/yeast/decoys.fa.gz"
  salmon_index: "references/yeast/salmon_index"
  auto_download: true  # Set to true to auto-download references if missing
fastqc:
  extra: ""
multiqc:
  title: "RNA-seq QC"
salmon:
  libtype: "A"
  extra: "--validateMappings --gcBias"
  threads: 4
r:
  design: "~ condition + batch"
  contrast_variable: "condition"
  batch_column: "batch"
  alpha: 0.05
  lfc_shrink: true
  contrasts_file: "config/contrasts.tsv"
  gene_id_column: "gene_id"
  pvalue_adjust: "BH"
fgsea:
  genesets: null
  min_size: 1
  max_size: 200
  nperm: 1000
  padj_cutoff: 0.1
  score_column: "log2FoldChange"
report:
  author: "Your Name"
  title: "RNA-seq Analysis Report"
  output_html: "results/report.html"
paths:
  samples: "config/samples.tsv"
  outdir: "results"
  logs: "logs"
  qc: "results/qc"
  salmon: "results/salmon"
  counts: "results/counts"
  de: "results/de"
  fgsea: "results/fgsea"
  report_dir: "results"
containers:
  enabled: true
  image: "docker://rnaseq-mini:latest"
cache:
  enabled: true
  dir: ".cache"
  max_age_days: 30
singlecell:
  enabled: false
  # Path to the directory containing the 10x formatted matrix, barcodes, and features files.
  # This is the primary input for the single-cell analysis workflow.
  input_dir: "path/to/your/10x_data"
  
  # Analysis parameters for the Scanpy workflow
  analysis:
    min_genes_per_cell: 200
    min_cells_per_gene: 3
    max_mito_percent: 15.0
    n_pcs: 30
    n_neighbors: 15
    clustering_resolution: 0.5

  # Automated cell type annotation with CellTypist
  annotation:
    enabled: true
    # Name of a pre-trained CellTypist model.
    # A list of available models can be found at: https://celltypist.st-jude.org/models
    model: "Immune_All_Low.pkl"

  # Output paths
  paths:
    output_dir: "results/singlecell/analysis"


# Dynamic resource optimization
optimization:
  enabled: false              # Enable dynamic resource optimization
  auto_estimate: true         # Automatically estimate resources from samples.tsv
  adaptive_scaling: true      # Enable adaptive scaling during execution

  # Resource estimation parameters
  estimation:
    base_cores: 4
    base_memory_gb: 16
    complexity_multiplier: 1.5
    size_multiplier: 1.2
    min_cores: 1
    max_cores: 32
    min_memory_gb: 4
    max_memory_gb: 128

  # Cloud optimization
  cloud:
    enabled: false
    provider: "aws"           # aws, gcp, azure
    region: "us-east-1"

    # Auto-scaling configuration
    autoscaling:
      enabled: true
      min_vcpus: 0
      max_vcpus: 256
      scale_up_threshold: 0.8   # Scale up when >80% utilization
      scale_down_threshold: 0.3  # Scale down when <30% utilization
      scale_up_factor: 1.5
      scale_down_factor: 0.7
      cooldown_minutes: 5

    # Cost monitoring
    cost_monitoring:
      enabled: true
      budget_alert_threshold: 0.8  # Alert when 80% of budget reached
      report_frequency_days: 7

# Resource allocation profiles
resource_profiles:
  small:    # < 10 samples, < 1GB total
    cores: 4
    memory_gb: 8
    runtime_hours: 2

  medium:   # 10-50 samples, 1-10GB total
    cores: 8
    memory_gb: 16
    runtime_hours: 6

  large:    # 50-200 samples, 10-50GB total
    cores: 16
    memory_gb: 32
    runtime_hours: 12

  xlarge:   # > 200 samples, > 50GB total
    cores: 32
    memory_gb: 64
    runtime_hours: 24

# Error handling and troubleshooting
error_handling:
  enabled: true              # Enable intelligent error handling
  auto_retry: true           # Automatically retry failed operations
  max_retries: 3            # Maximum retry attempts
  retry_delay: 30           # Delay between retries (seconds)
  solution_timeout: 300     # Timeout for solution execution (seconds)

troubleshooting:
  health_check_interval: 30  # Health check interval (seconds)
  auto_fix_enabled: true     # Enable automatic fixes
  diagnostic_level: "comprehensive"  # basic, standard, comprehensive
  report_format: "markdown"  # markdown, html, json

profiles:
  snakemake: "config/profiles/local.smk.yaml"
  nextflow: "config/profiles/local.nf.config"

# Reference genome parameters
# `organism_name` should match a key in `config/genome.yaml` (e.g., "yeast", "human")
organism_name: "yeast"

# Automated reference management
# If enabled, the pipeline will attempt to download the required reference files
# and build the Salmon index if they are not found at the paths specified in
# `config/genome.yaml`.
references:
  auto_download: true
  # The `species` key should match one of the presets in `scripts/download_references.py`
  # (e.g., "yeast", "human", "mouse").
  species: "yeast"

# Input/output directories
fastq_dir: "tests/data/fastq/"

# --- Dynamic Resource Allocation ---
resource_profiles:
  # Default resources for jobs if not otherwise specified
  default: &default_resources
    threads: 2
    mem_gb: 4

  # Resources for specific rules/processes
  fastqc:
    small: { threads: 1, mem_gb: 2 }
    medium: { threads: 2, mem_gb: 4 }
    large: { threads: 4, mem_gb: 8 }
  
  salmon_quant:
    small: { threads: 2, mem_gb: 8 }
    medium: { threads: 4, mem_gb: 16 }
    large: { threads: 8, mem_gb: 32 }

  deseq2:
    small: { threads: 2, mem_gb: 8 }
    medium: { threads: 4, mem_gb: 16 }
    large: { threads: 8, mem_gb: 32 }

  # Thresholds (in bytes) for determining which profile to use based on FASTQ file size
  size_thresholds:
    small: 100000000   # 100 MB
    medium: 1000000000  # 1 GB
    # Anything larger than 'medium' is considered 'large'
